ğŸ’¡ Como as ferramentas Streamlit, MLflow, PyCaret e Scikit-Learn auxiliam na construÃ§Ã£o dos pipelines?

Este projeto integra ferramentas modernas de MLOps e ciÃªncia de dados para facilitar o desenvolvimento, rastreamento, monitoramento e deploy de modelos de Machine Learning. Abaixo estÃ£o os papÃ©is principais de cada ferramenta nos pipelines:

âœ… Rastreamento de Experimentos â€“ MLflow + PyCaret
MLflow permite o rastreamento completo dos experimentos com registro automÃ¡tico de:

HiperparÃ¢metros

MÃ©tricas (ex: AcurÃ¡cia, F1-Score, Log Loss)

Artefatos como modelos treinados, grÃ¡ficos e arquivos de avaliaÃ§Ã£o

PyCaret se integra com MLflow e registra automaticamente todos os experimentos executados, facilitando a comparaÃ§Ã£o entre diferentes abordagens de modelagem.

âš™ï¸ FunÃ§Ãµes de Treinamento â€“ PyCaret + Scikit-Learn
PyCaret simplifica o ciclo completo de modelagem com automaÃ§Ã£o de:

PrÃ©-processamento

SeleÃ§Ã£o e tuning de modelos

ValidaÃ§Ã£o cruzada

Scikit-Learn Ã© utilizado para criaÃ§Ã£o de pipelines customizados e aplicaÃ§Ã£o de tÃ©cnicas avanÃ§adas de feature engineering, garantindo flexibilidade e controle total do processo.

ğŸ“Š Monitoramento da SaÃºde do Modelo â€“ Streamlit + MLflow
Streamlit Ã© utilizado para criar dashboards interativos e acompanhar:

MÃ©tricas de performance dos modelos em tempo real

ComparaÃ§Ãµes entre modelos com visualizaÃ§Ãµes claras (ex: Matriz de ConfusÃ£o)

MLflow complementa com logs histÃ³ricos e versÃµes de modelos, facilitando a auditoria e manutenÃ§Ã£o.

ğŸ”„ AtualizaÃ§Ã£o de Modelo â€“ MLflow + Pipelines AutomÃ¡ticos
O pipeline verifica e registra novos modelos com MLflow.

Caso um novo modelo tenha performance superior, ele Ã© promovido como o novo modelo de produÃ§Ã£o.

Todo o histÃ³rico de versÃµes Ã© preservado, permitindo rollback se necessÃ¡rio.

ğŸš€ Provisionamento (Deployment) â€“ Streamlit + MLflow
Streamlit serve como interface de inferÃªncia interativa para usuÃ¡rios e analistas.

MLflow possibilita servir o modelo como API REST (mlflow serve), permitindo integraÃ§Ã£o com outras aplicaÃ§Ãµes e sistemas.

O uso de modelos registrados garante reprodutibilidade e consistÃªncia entre os ambientes de teste e produÃ§Ã£o.

ğŸ“Œ 1. O modelo Ã© aderente a essa nova base? O que mudou entre uma base e outra? Justifique.
Sim, o modelo Ã© aderente Ã  nova base, desde que ela mantenha o mesmo esquema de features esperadas pelo pipeline (mesmas colunas, tipos de dados e prÃ©-processamento aplicado).

âœ… O que mudou entre uma base e outra?
Base de treino (histÃ³rica): usada para treinar e validar os modelos, com a variÃ¡vel resposta shot_made_flag disponÃ­vel.

Base de produÃ§Ã£o (nova): pode ou nÃ£o conter a variÃ¡vel resposta. Nessa nova base:

Pode haver drift nos dados, ou seja, mudanÃ§as nas distribuiÃ§Ãµes das variÃ¡veis.

Pode haver novas combinaÃ§Ãµes de variÃ¡veis categÃ³ricas, ou valores faltantes inesperados.

Pode haver reduÃ§Ã£o de qualidade das features, caso o pipeline nÃ£o tenha sido aplicado corretamente.

ğŸ” Para garantir aderÃªncia, Ã© importante usar pipelines robustos e versionados, que transformem os dados novos da mesma forma que os dados histÃ³ricos foram tratados.

ğŸ©º 2. Como monitorar a saÃºde do modelo com e sem a variÃ¡vel resposta?
âœ… Com a variÃ¡vel resposta disponÃ­vel
Quando a variÃ¡vel shot_made_flag estÃ¡ disponÃ­vel na base de produÃ§Ã£o (mesmo que com atraso), Ã© possÃ­vel:

Calcular mÃ©tricas de performance reais (ex: AcurÃ¡cia, F1, Log Loss).

Comparar previsÃµes com os valores reais.

Detectar degradaÃ§Ã£o de performance ao longo do tempo (monitoramento contÃ­nuo com MLflow e dashboards Streamlit).

Atualizar ou reverter modelos se necessÃ¡rio, com base em performance real.

Ferramentas como MLflow + Streamlit ajudam a visualizar isso com dashboards operacionais.

âš ï¸ Sem a variÃ¡vel resposta
Quando a variÃ¡vel resposta nÃ£o estÃ¡ disponÃ­vel no momento da prediÃ§Ã£o:

Utilizamos monitoramento indireto:

DistribuiÃ§Ã£o das previsÃµes (ex: distribuiÃ§Ã£o dos scores ou classes previstas).

Drift de dados de entrada (ex: Kolmogorov-Smirnov test entre a base de treino e a base nova).

Monitoramento de outliers ou mudanÃ§as de perfil no input.

Ferramentas como EvidentlyAI ou cÃ³digo customizado podem calcular mÃ©tricas de drift, como:

Population Stability Index (PSI)

Jensen-Shannon Distance

Essas tÃ©cnicas ajudam a antecipar problemas, mesmo sem feedback imediato.

ğŸ”„ 3. EstratÃ©gias de Retreinamento â€“ Reativa e Preditiva
ğŸ§¯ EstratÃ©gia Reativa
Consiste em reagir Ã  degradaÃ§Ã£o de performance do modelo:

Ã‰ aplicada quando a variÃ¡vel resposta se torna disponÃ­vel.

Monitoramos mÃ©tricas como F1, Log Loss, AcurÃ¡cia no tempo.

Retreinamento Ã© disparado quando o desempenho cai abaixo de um limite aceitÃ¡vel.

Pode ser agendado (ex: mensal) ou baseado em gatilhos (ex: F1 < 0.7).

ğŸŸ  PrÃ³s: Simples de implementar; baseado em feedback real.
ğŸ”µ Contras: A performance pode jÃ¡ estar ruim antes do retreinamento.

ğŸ”® EstratÃ©gia Preditiva
Consiste em prever quando serÃ¡ necessÃ¡rio retreinar, mesmo sem a variÃ¡vel resposta:

Baseada em mudanÃ§as nos dados de entrada (Data Drift ou Concept Drift).

Utiliza indicadores como:

MudanÃ§a no perfil dos dados

MudanÃ§a na distribuiÃ§Ã£o das previsÃµes

Alertas baseados em regras (thresholds)

Pode usar modelos auxiliares para detectar anomalias ou instabilidades.

ğŸŸ¢ PrÃ³s: AntecipaÃ§Ã£o de problemas; evita queda brusca de performance.
ğŸ”´ Contras: Pode gerar falsos positivos e retreinamentos desnecessÃ¡rios.

